{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e35d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set up the environment. Code sources are in folders which are in the parent folder of this notebooks scope.\n",
    "import sys; sys.path.insert(0, '..') # add parent folder path, now files are queriable from parent folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4fb1e9",
   "metadata": {},
   "source": [
    "Load the data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930b475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.query.util import loadJson\n",
    "\n",
    "notHitSongPath = '../data/datasets/spotify/not_hit_song.json'\n",
    "hitSongPath = '../data/datasets/spotify/hit_song.json'\n",
    "\n",
    "\n",
    "hits = loadJson(hitSongPath)\n",
    "notHits = loadJson(notHitSongPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1385b633",
   "metadata": {},
   "source": [
    "Sneak peak on the structure of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05978026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'features'])\n",
      "dict_keys(['spotifyData', 'searchQuery', 'minMatchingRatioUsed', 'originalData'])\n",
      "dict_keys(['timeSignature', 'durationMS', 'key', 'mode', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'valence', 'tempo'])\n"
     ]
    }
   ],
   "source": [
    "print(hits[0].keys())\n",
    "print(hits[0]['info'].keys())\n",
    "print(hits[0]['features'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cec1f3a",
   "metadata": {},
   "source": [
    "Now all hits will be added a label 1 and not hits are labeled as 0. 0s and 1s are used because the model needs a numerical input as the correct label when training. For example in MLP model (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) the weights of the model are adjusted so that the output will be closer to the label value the model has been given.\n",
    "\n",
    "This is called **binary classification**, essentially the numerical output for a song ( spotify feature values as input) from the model is used to classify the song of being a hit > 0.5 or not < 0.5.\n",
    "\n",
    "In the scope of this modeling a song can only be a hit or a not hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac9a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hitsTotal, hit in enumerate(hits):\n",
    "    hit['labels'] = {'hit': 1}\n",
    "\n",
    "for notHitsTotal, notHit in enumerate(notHits):\n",
    "    notHit['labels'] = {'hit': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93e082d",
   "metadata": {},
   "source": [
    "To show some plots about the data **matplotlib**s pyplot need to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0efb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def makeHistogram(labels, data):\n",
    "    plt.bar(labels, data)\n",
    "    plt.xticks(range(1, len(labels)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc106861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19463\n",
      "83775\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAT9UlEQVR4nO3dYYhdZ37f8e8vUjar3VSO7R0bVRKVisW2smG9tVDVLpS2SmulWyK/sEELqUURqBi3TUqhyH2z9IXAhlK3htog4q1lN11bVbJYJDiNkLOEgpAz3nXrlb3C0/XGmkq1JmvHcRqsVO6/L+5/6J3x1cydkazRrr4fuJxz/+d5zjwHBL97nnOvnlQVkiT91EoPQJJ0fTAQJEmAgSBJagaCJAkwECRJbfVKD2C5vvCFL9SmTZtWehiS9GPl1Vdf/cOqmhh17Mc2EDZt2sTk5ORKD0OSfqwk+YPLHXPKSJIEGAiSpGYgSJIAA0GS1AwESRJgIEiSmoEgSQIMBElSMxAkScCP8S+Vr8SmA7+10kPQdeyHj351pYcgrQjvECRJgIEgSWoGgiQJMBAkSc1AkCQBYwZCkn+W5HSS7yX5ZpLPJrklyfEkb/X25qH2jySZSnImyb1D9XuSvN7HnkiSrv9Mkhe6firJpqt+pZKkBS0aCEnWA/8U2FZVdwGrgD3AAeBEVW0BTvR7kmzt43cCu4Ank6zq0z0F7Ae29GtX1/cB71fVHcDjwGNX5eokSWMbd8poNbAmyWrgc8A5YDdwuI8fBu7r/d3A81V1sareBqaA7UnWAWur6mRVFfDsvD6z5zoK7Jy9e5AkXRuLBkJV/U/gXwPvAOeBD6rqd4Dbq+p8tzkP3NZd1gNnh04x3bX1vT+/PqdPVV0CPgBunT+WJPuTTCaZnJmZGfcaJUljGGfK6GYGn+A3A38e+HySX1qoy4haLVBfqM/cQtWhqtpWVdsmJkauES1JWqZxpox+Hni7qmaq6v8AvwH8deDdngaitxe6/TSwcaj/BgZTTNO9P78+p09PS90EvLecC5IkLc84gfAOsCPJ53pefyfwJnAM2Ntt9gIv9v4xYE9/c2gzg4fHr/S00odJdvR5HpzXZ/Zc9wMv93MGSdI1suh/bldVp5IcBb4DXAK+CxwCfhY4kmQfg9B4oNufTnIEeKPbP1xVH/fpHgKeAdYAL/UL4GnguSRTDO4M9lyVq5MkjW2s/+20qr4OfH1e+SKDu4VR7Q8CB0fUJ4G7RtQ/ogNFkrQy/KWyJAkwECRJzUCQJAEGgiSpGQiSJMBAkCQ1A0GSBBgIkqRmIEiSAANBktQMBEkSYCBIkpqBIEkCDARJUjMQJEmAgSBJaosGQpIvJnlt6PXHSX4lyS1Jjid5q7c3D/V5JMlUkjNJ7h2q35Pk9T72RC+lSS+3+ULXTyXZ9KlcrSTpshYNhKo6U1V3V9XdwD3AnwLfAg4AJ6pqC3Ci35NkK4MlMO8EdgFPJlnVp3sK2M9gneUtfRxgH/B+Vd0BPA48dlWuTpI0tqVOGe0E/kdV/QGwGzjc9cPAfb2/G3i+qi5W1dvAFLA9yTpgbVWdrKoCnp3XZ/ZcR4Gds3cPkqRrY6mBsAf4Zu/fXlXnAXp7W9fXA2eH+kx3bX3vz6/P6VNVl4APgFvn//Ek+5NMJpmcmZlZ4tAlSQsZOxCSfAb4ReA/L9Z0RK0WqC/UZ26h6lBVbauqbRMTE4sMQ5K0FEu5Q/gF4DtV9W6/f7engejtha5PAxuH+m0AznV9w4j6nD5JVgM3Ae8tYWySpCu0lED4Gv9/ugjgGLC39/cCLw7V9/Q3hzYzeHj8Sk8rfZhkRz8feHBen9lz3Q+83M8ZJEnXyOpxGiX5HPB3gH80VH4UOJJkH/AO8ABAVZ1OcgR4A7gEPFxVH3efh4BngDXAS/0CeBp4LskUgzuDPVdwTZKkZRgrEKrqT5n3kLeqfsTgW0ej2h8EDo6oTwJ3jah/RAeKJGll+EtlSRJgIEiSmoEgSQIMBElSMxAkSYCBIElqBoIkCTAQJEnNQJAkAQaCJKkZCJIkwECQJDUDQZIEGAiSpGYgSJKAMQMhyc8lOZrk+0neTPLXktyS5HiSt3p781D7R5JMJTmT5N6h+j1JXu9jT/TKafTqai90/VSSTVf9SiVJCxr3DuHfAb9dVX8J+BLwJnAAOFFVW4AT/Z4kWxmseHYnsAt4MsmqPs9TwH4Gy2pu6eMA+4D3q+oO4HHgsSu8LknSEi0aCEnWAn+DwTKXVNWfVdUfAbuBw93sMHBf7+8Gnq+qi1X1NjAFbE+yDlhbVSd7veRn5/WZPddRYOfs3YMk6doY5w7hLwIzwH9I8t0kv5rk88DtVXUeoLe3dfv1wNmh/tNdW9/78+tz+lTVJeAD5i3ZCZBkf5LJJJMzMzNjXqIkaRzjBMJq4K8AT1XVl4H/TU8PXcaoT/a1QH2hPnMLVYeqaltVbZuYmFh41JKkJRknEKaB6ao61e+PMgiId3saiN5eGGq/caj/BuBc1zeMqM/pk2Q1cBPw3lIvRpK0fIsGQlX9L+Bski92aSfwBnAM2Nu1vcCLvX8M2NPfHNrM4OHxKz2t9GGSHf184MF5fWbPdT/wcj9nkCRdI6vHbPdPgF9L8hngB8A/ZBAmR5LsA94BHgCoqtNJjjAIjUvAw1X1cZ/nIeAZYA3wUr9g8MD6uSRTDO4M9lzhdUmSlmisQKiq14BtIw7tvEz7g8DBEfVJ4K4R9Y/oQJEkrQx/qSxJAgwESVIzECRJgIEgSWoGgiQJMBAkSc1AkCQBBoIkqRkIkiTAQJAkNQNBkgQYCJKkZiBIkgADQZLUDARJEmAgSJLaWIGQ5IdJXk/yWpLJrt2S5HiSt3p781D7R5JMJTmT5N6h+j19nqkkT/RSmvRymy90/VSSTVf5OiVJi1jKHcLfqqq7q2p25bQDwImq2gKc6Pck2cpgCcw7gV3Ak0lWdZ+ngP0M1lne0scB9gHvV9UdwOPAY8u/JEnSclzJlNFu4HDvHwbuG6o/X1UXq+ptYArYnmQdsLaqTlZVAc/O6zN7rqPAztm7B0nStTFuIBTwO0leTbK/a7dX1XmA3t7W9fXA2aG+011b3/vz63P6VNUl4APg1vmDSLI/yWSSyZmZmTGHLkkax+ox232lqs4luQ04nuT7C7Qd9cm+Fqgv1GduoeoQcAhg27ZtnzguSVq+se4Qqupcby8A3wK2A+/2NBC9vdDNp4GNQ903AOe6vmFEfU6fJKuBm4D3ln45kqTlWjQQknw+yZ+b3Qf+LvA94Biwt5vtBV7s/WPAnv7m0GYGD49f6WmlD5Ps6OcDD87rM3uu+4GX+zmDJOkaGWfK6HbgW/2MdzXwn6rqt5P8PnAkyT7gHeABgKo6neQI8AZwCXi4qj7ucz0EPAOsAV7qF8DTwHNJphjcGey5CtcmSVqCRQOhqn4AfGlE/UfAzsv0OQgcHFGfBO4aUf+IDhRJ0srwl8qSJMBAkCQ1A0GSBBgIkqRmIEiSAANBktQMBEkSYCBIkpqBIEkCDARJUjMQJEmAgSBJagaCJAkwECRJzUCQJAEGgiSpjR0ISVYl+W6S3+z3tyQ5nuSt3t481PaRJFNJziS5d6h+T5LX+9gTvZQmvdzmC10/lWTTVbxGSdIYlnKH8MvAm0PvDwAnqmoLcKLfk2QrgyUw7wR2AU8mWdV9ngL2M1hneUsfB9gHvF9VdwCPA48t62okScs2ViAk2QB8FfjVofJu4HDvHwbuG6o/X1UXq+ptYArYnmQdsLaqTlZVAc/O6zN7rqPAztm7B0nStTHuHcK/Bf4F8H+HardX1XmA3t7W9fXA2aF2011b3/vz63P6VNUl4APg1vmDSLI/yWSSyZmZmTGHLkkax6KBkOTvAxeq6tUxzznqk30tUF+oz9xC1aGq2lZV2yYmJsYcjiRpHKvHaPMV4BeT/D3gs8DaJP8ReDfJuqo639NBF7r9NLBxqP8G4FzXN4yoD/eZTrIauAl4b5nXJElahkXvEKrqkaraUFWbGDwsfrmqfgk4BuztZnuBF3v/GLCnvzm0mcHD41d6WunDJDv6+cCD8/rMnuv+/hufuEOQJH16xrlDuJxHgSNJ9gHvAA8AVNXpJEeAN4BLwMNV9XH3eQh4BlgDvNQvgKeB55JMMbgz2HMF45IkLcOSAqGqvg18u/d/BOy8TLuDwMER9UngrhH1j+hAkSStDH+pLEkCDARJUjMQJEmAgSBJagaCJAkwECRJzUCQJAEGgiSpGQiSJMBAkCQ1A0GSBBgIkqRmIEiSAANBktQMBEkSMN6ayp9N8kqS/5bkdJJ/1fVbkhxP8lZvbx7q80iSqSRnktw7VL8nyet97IleOY1eXe2Frp9KsulTuFZJ0gLGuUO4CPztqvoScDewK8kO4ABwoqq2ACf6PUm2Mljx7E5gF/BkklV9rqeA/QyW1dzSxwH2Ae9X1R3A48BjV35pkqSlGGdN5aqqP+m3P92vAnYDh7t+GLiv93cDz1fVxap6G5gCtidZB6ytqpO9XvKz8/rMnusosHP27kGSdG2M9QwhyaokrwEXgONVdQq4varOA/T2tm6+Hjg71H26a+t7f359Tp+qugR8ANy6jOuRJC3TWIFQVR9X1d3ABgaf9j+xLvKQUZ/sa4H6Qn3mnjjZn2QyyeTMzMwio5YkLcWSvmVUVX8EfJvB3P+7PQ1Eby90s2lg41C3DcC5rm8YUZ/TJ8lq4CbgvRF//1BVbauqbRMTE0sZuiRpEeN8y2giyc/1/hrg54HvA8eAvd1sL/Bi7x8D9vQ3hzYzeHj8Sk8rfZhkRz8feHBen9lz3Q+83M8ZJEnXyOox2qwDDvc3hX4KOFJVv5nkJHAkyT7gHeABgKo6neQI8AZwCXi4qj7ucz0EPAOsAV7qF8DTwHNJphjcGey5GhcnSRrfooFQVf8d+PKI+o+AnZfpcxA4OKI+CXzi+UNVfUQHiiRpZfhLZUkSYCBIkpqBIEkCDARJUjMQJEmAgSBJagaCJAkwECRJzUCQJAEGgiSpGQiSJMBAkCQ1A0GSBBgIkqRmIEiSAANBktTGWUJzY5LfTfJmktNJfrnrtyQ5nuSt3t481OeRJFNJziS5d6h+T5LX+9gTvZQmvdzmC10/lWTTp3CtkqQFjHOHcAn451X1l4EdwMNJtgIHgBNVtQU40e/pY3uAO4FdwJO9/CbAU8B+Bussb+njAPuA96vqDuBx4LGrcG2SpCVYNBCq6nxVfaf3PwTeBNYDu4HD3ewwcF/v7waer6qLVfU2MAVsT7IOWFtVJ6uqgGfn9Zk911Fg5+zdgyTp2ljSM4SeyvkycAq4varOwyA0gNu62Xrg7FC36a6t7/359Tl9quoS8AFw64i/vz/JZJLJmZmZpQxdkrSIsQMhyc8Cvw78SlX98UJNR9RqgfpCfeYWqg5V1baq2jYxMbHYkCVJS7B6nEZJfppBGPxaVf1Gl99Nsq6qzvd00IWuTwMbh7pvAM51fcOI+nCf6SSrgZuA95ZxPdJPhE0Hfmulh6Dr2A8f/eqnct5xvmUU4Gngzar6N0OHjgF7e38v8OJQfU9/c2gzg4fHr/S00odJdvQ5H5zXZ/Zc9wMv93MGSdI1Ms4dwleAfwC8nuS1rv1L4FHgSJJ9wDvAAwBVdTrJEeANBt9QeriqPu5+DwHPAGuAl/oFg8B5LskUgzuDPVd2WZKkpVo0EKrqvzJ6jh9g52X6HAQOjqhPAneNqH9EB4okaWX4S2VJEmAgSJKagSBJAgwESVIzECRJgIEgSWoGgiQJMBAkSc1AkCQBBoIkqRkIkiTAQJAkNQNBkgQYCJKkZiBIkgADQZLUxllC8xtJLiT53lDtliTHk7zV25uHjj2SZCrJmST3DtXvSfJ6H3uil9Gkl9p8oeunkmy6ytcoSRrDOHcIzwC75tUOACeqagtwot+TZCuD5S/v7D5PJlnVfZ4C9jNYY3nL0Dn3Ae9X1R3A48Bjy70YSdLyLRoIVfV7DNY5HrYbONz7h4H7hurPV9XFqnobmAK2J1kHrK2qk1VVwLPz+sye6yiwc/buQZJ07Sz3GcLtVXUeoLe3dX09cHao3XTX1vf+/PqcPlV1CfgAuHXUH02yP8lkksmZmZllDl2SNMrVfqg86pN9LVBfqM8ni1WHqmpbVW2bmJhY5hAlSaMsNxDe7Wkgenuh69PAxqF2G4BzXd8woj6nT5LVwE18copKkvQpW24gHAP29v5e4MWh+p7+5tBmBg+PX+lppQ+T7OjnAw/O6zN7rvuBl/s5gyTpGlq9WIMk3wT+JvCFJNPA14FHgSNJ9gHvAA8AVNXpJEeAN4BLwMNV9XGf6iEG31haA7zUL4CngeeSTDG4M9hzVa5MkrQkiwZCVX3tMod2Xqb9QeDgiPokcNeI+kd0oEiSVo6/VJYkAQaCJKkZCJIkwECQJDUDQZIEGAiSpGYgSJIAA0GS1AwESRJgIEiSmoEgSQIMBElSMxAkSYCBIElqBoIkCTAQJEntugmEJLuSnEkyleTASo9Hkm4010UgJFkF/HvgF4CtwNeSbF3ZUUnSjeW6CARgOzBVVT+oqj8Dngd2r/CYJOmGsuiaytfIeuDs0Ptp4K/Ob5RkP7C/3/5JkjPXYGy6weSxlR6BRvgC8IcrPYjrxRX+G/0LlztwvQRCRtTqE4WqQ8ChT384kq4nSSarattKj+Mn3fUyZTQNbBx6vwE4t0JjkaQb0vUSCL8PbEmyOclngD3AsRUekyTdUK6LKaOqupTkHwP/BVgFfKOqTq/wsCRdP5wqvgZS9YmpeknSDeh6mTKSJK0wA0GSBBgIkq5jSb6R5EKS7630WG4EBoKk69kzwK6VHsSNwkCQdN2qqt8D3lvpcdwoDARJEmAgSJKagSBJAgwESVIzECRdt5J8EzgJfDHJdJJ9Kz2mn2T+1xWSJMA7BElSMxAkSYCBIElqBoIkCTAQJEnNQJAkAQaCJKn9PyQT6lxXAdBgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(hitsTotal)\n",
    "print(notHitsTotal)\n",
    "\n",
    "makeHistogram([0, 1], [notHitsTotal, hitsTotal])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e84010f",
   "metadata": {},
   "source": [
    "As this majestic histogram shows, there are a lot of not hits compared to hits.\n",
    "\n",
    "To efficiently train the model, data needs to be balanced. Here is a good article about the subject https://medium.com/analytics-vidhya/what-is-balance-and-imbalance-dataset-89e8d7f46bc5\n",
    "\n",
    "There are a few tricks to apply the balancing with, but here I chose the **under sampling** of not hit songs. The amount of hits (almost 20k) is sufficient amount of data in my papers, so if the not hit dataset is under sampled to match this amount the result is a balanced dataset with 40k songs.\n",
    "\n",
    "If the undersampling is done totally randomly, there's a possibility to have another imbalanced datset on the not hit side. The sampling can end up choosing most of the songs before 70s for example. This would just manifest the original problem in a different form. So to ensure this doesn't happen, the samples need to take same amount in every time period. Year is a good enough measure for it and that information is available in the song meta data under albums release date. This doesn't necessarily tell when exactly the song has been released, but it's a good enough measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.types.spotify import SpotifySongData\n",
    "from data.types.process import SongFeaturesAndLabels, ModelFeatures\n",
    "\n",
    "def parseYearFromDate(date: str) -> int:\n",
    "    if '-' in date:\n",
    "        split = date.split('-')\n",
    "        if len(split) == 3:\n",
    "            year, month, day = split\n",
    "        elif len(split) == 2:\n",
    "            year, month = split\n",
    "        return year\n",
    "            \n",
    "    elif len(date) == 4:\n",
    "        return date\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def parseToFeaturesAndLabels(songs: list[SpotifySongData]) -> list[SongFeaturesAndLabels]:\n",
    "    featuresAndLabels: list[SongFeaturesAndLabels] = []\n",
    "    for songData in songs:\n",
    "        features = songData['features']\n",
    "        releaseDate = parseYearFromDate(songData['info']['spotifyData']['album']['releaseDate'])\n",
    "\n",
    "        if releaseDate == 0 or len(features.values()) != 13:\n",
    "            # If there's no date ignore the song\n",
    "            continue\n",
    "            \n",
    "        modelFeatures: ModelFeatures = {\n",
    "            **features, \n",
    "            'releaseYear': releaseDate\n",
    "        }\n",
    "        label = songData['labels']['hit']\n",
    "        \n",
    "        songFeaturesAndLabels: SongFeaturesAndLabels = {\n",
    "            'spotifyID': songData['info']['spotifyData']['songID'],\n",
    "            'features': modelFeatures,\n",
    "            'label': label\n",
    "        }\n",
    "        featuresAndLabels.append(songFeaturesAndLabels)\n",
    "        \n",
    "    return featuresAndLabels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e186edef",
   "metadata": {},
   "source": [
    "With hit songs it is straight forward. The songs doesn't need any sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dcc248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "parsedHitSongs = parseToFeaturesAndLabels(hits)\n",
    "parsedNotHitSongs = parseToFeaturesAndLabels(notHits)\n",
    "\n",
    "print(\"HITS\")\n",
    "# Plot histogram where amount of hits are by release date\n",
    "#print(sorted(Counter([song['features']['releaseYear'] for song in parsedHitSongs]).items(), key=lambda x: x[1]))\n",
    "labels, amounts = zip(*sorted(Counter([song['features']['releaseYear'] for song in parsedHitSongs]).items(), key=lambda x: x[1]))\n",
    "makeHistogram(labels, amounts)\n",
    "print(\"NOT HITS\")\n",
    "# Not hit hist\n",
    "nlabels, namounts = zip(*sorted(Counter([song['features']['releaseYear'] for song in parsedNotHitSongs]).items(), key=lambda x: x[1]))\n",
    "makeHistogram(nlabels, namounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a090f5f9",
   "metadata": {},
   "source": [
    "The data is quite unbalanced between release years.\n",
    "\n",
    "And split with decade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "decades = []\n",
    "for song in parsedHitSongs:\n",
    "    year = song['features']['releaseYear']    \n",
    "    if year > '1939':\n",
    "        # Dropping songs released before 1939\n",
    "        decades.append(year[2]+\"0\")\n",
    "\n",
    "print(\"HITS\")\n",
    "#print(sorted(Counter(decades).items(), key=lambda x: x[1]))\n",
    "labels, amounts = zip(*sorted(Counter(decades).items(), key=lambda x: x[1]))\n",
    "#print(amounts[:10], labels[:10])\n",
    "makeHistogram(labels, amounts)\n",
    "\n",
    "ndecades = []\n",
    "for song in parsedNotHitSongs:\n",
    "    year = song['features']['releaseYear']    \n",
    "    if year > '1939':\n",
    "        # Dropping songs released before 1939\n",
    "        ndecades.append(year[2]+\"0\")\n",
    "        \n",
    "print(\"NOT HITS\")\n",
    "nlabels, namounts = zip(*sorted(Counter(ndecades).items(), key=lambda x: x[1]))\n",
    "#print(amounts[:10], labels[:10])\n",
    "makeHistogram(nlabels, namounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba859b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(decades))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be5caff",
   "metadata": {},
   "source": [
    "Alright so the hit songs aren't evenly distrubuted among years. This means that some additional filtering can be helpful.\n",
    "\n",
    "In this situation it looks like the best way to sample the songs is: \n",
    " 1. Ignore all songs released in the 50s.\n",
    " 2. Take from every decade the same amount of songs that is released in the 20s\n",
    " \n",
    "There are 1107 songs in the data that's released in the 20s so if we take this amount of songs from every decade, the final model will be trained with a dataset of 7 decades * 1107 = 7749 hit songs, so dataset of 15 498 songs in total.\n",
    "\n",
    "This is quite a small dataset in size, but it's enough to see if there's something to find.\n",
    "\n",
    "Here's the decade sampling implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d8bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checkpoint\n",
    "import sys; sys.path.insert(0, '..') # add parent folder path, now files are queriable from parent folder\n",
    "\n",
    "from data.query.util import loadJson\n",
    "\n",
    "notHitSongPath = '../data/datasets/spotify/not_hit_song.json'\n",
    "hitSongPath = '../data/datasets/spotify/hit_song.json'\n",
    "\n",
    "\n",
    "hits = loadJson(hitSongPath)\n",
    "notHits = loadJson(notHitSongPath)\n",
    "\n",
    "for hitsTotal, hit in enumerate(hits):\n",
    "    hit['labels'] = {'hit': 1}\n",
    "\n",
    "for notHitsTotal, notHit in enumerate(notHits):\n",
    "    notHit['labels'] = {'hit': 0}\n",
    "    \n",
    "from data.types.spotify import SpotifySongData\n",
    "from data.types.process import SongFeaturesAndLabels, ModelFeatures\n",
    "\n",
    "def parseYearFromDate(date: str) -> int:\n",
    "    if '-' in date:\n",
    "        split = date.split('-')\n",
    "        if len(split) == 3:\n",
    "            year, month, day = split\n",
    "        elif len(split) == 2:\n",
    "            year, month = split\n",
    "        return year\n",
    "            \n",
    "    elif len(date) == 4:\n",
    "        return date\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def parseToFeaturesAndLabels(songs: list[SpotifySongData]) -> list[SongFeaturesAndLabels]:\n",
    "    featuresAndLabels: list[SongFeaturesAndLabels] = []\n",
    "    for songData in songs:\n",
    "        features = songData['features']\n",
    "        releaseDate = parseYearFromDate(songData['info']['spotifyData']['album']['releaseDate'])\n",
    "        if releaseDate == 0 or len(features.values()) != 13:\n",
    "            # If there's no date ignore the song\n",
    "            continue\n",
    "            \n",
    "        modelFeatures: ModelFeatures = {\n",
    "            **features, \n",
    "            'releaseYear': releaseDate\n",
    "        }\n",
    "        label = songData['labels']['hit']\n",
    "        \n",
    "        songFeaturesAndLabels: SongFeaturesAndLabels = {\n",
    "            'spotifyID': songData['info']['spotifyData']['songID'],\n",
    "            'features': modelFeatures,\n",
    "            'label': label\n",
    "        }\n",
    "        featuresAndLabels.append(songFeaturesAndLabels)\n",
    "        \n",
    "    return featuresAndLabels\n",
    "\n",
    "parsedHitSongs = parseToFeaturesAndLabels(hits)\n",
    "parsedNotHitSongs = parseToFeaturesAndLabels(notHits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d81621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To figure out if there's a need to keep track of duplicates\n",
    "\n",
    "# Collect all hit and not hit song spotify ids (unique)\n",
    "hitSpotiIds = [song['spotifyID'] for song in parsedHitSongs]\n",
    "# Using Counter and filter take all spotify hit ids that are duplicates\n",
    "duplicateHits = dict(filter(lambda x: x[1] > 1, Counter(hitSpotiIds).items()))\n",
    "\n",
    "# Same operations to not hits\n",
    "notHitSpotiIds = [song['spotifyID'] for song in parsedNotHitSongs]\n",
    "duplicateNotHits = dict(filter(lambda x: x[1] > 1, Counter(notHitSpotiIds).items()))\n",
    "\n",
    "print(\"Hit amount %d from which uniqes %d duplicates %d\" % (len(hitSpotiIds), len(set(list(hitSpotiIds))), len(hitSpotiIds)-len(set(list(hitSpotiIds)))))\n",
    "print(\"Not hit amount %d from which uniqes %d duplicates %d\" % (len(notHitSpotiIds), len(set(list(notHitSpotiIds))), len(notHitSpotiIds)-len(set(list(notHitSpotiIds)))))\n",
    "print(\"\\nTotal amount %d from which uniqes %d duplicates %d\" % (len(notHitSpotiIds+hitSpotiIds), len(set(list(notHitSpotiIds+hitSpotiIds))), len(notHitSpotiIds+hitSpotiIds)-len(set(list(notHitSpotiIds+hitSpotiIds)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1727f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(duplicateHits, \"\\n\")\n",
    "print(duplicateNotHits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d4622",
   "metadata": {},
   "source": [
    "Hmm... there are multiple duplicates. These songs need to be cleared.\n",
    "Basically there's a few cases:\n",
    "\n",
    "1. Hit song is twice or more times in hit song list\n",
    "2. Not hit song is twice or more times in not hit song list\n",
    "3. Hit song is accidentially in not hit song list\n",
    "\n",
    "Not hit songs were collected by randomly selecting songs from an album that the hit song was pointing in spotify data. Therefore if another hit song was on that same album it can be wrongly added also as not hit as there was no checking in this step.\n",
    "\n",
    "All these duplicates can be just removed by making a dictionary with spotify id as the key. Then if there's duplicate it will not be added two times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc371fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the songs into decades\n",
    "\n",
    "hitSpotiIds = [song['spotifyID'] for song in parsedHitSongs]\n",
    "\n",
    "# By assigning the songs in dictionaries where key is spotify id, the duplicates will be dropped\n",
    "# with no hits the id needs to be checked not belonging to hit ids\n",
    "hitSongsInDecades = {\n",
    "    '10': {},\n",
    "    '20': {},\n",
    "    '60': {},\n",
    "    '70': {},\n",
    "    '80': {},\n",
    "    '90': {},\n",
    "    '00': {},\n",
    "}\n",
    "for song in parsedHitSongs:\n",
    "    year = song['features']['releaseYear']    \n",
    "    if year >= '1960':\n",
    "        # Dropping songs released before 60s\n",
    "        hitSongsInDecades[year[2]+\"0\"][song['spotifyID']] = (song['features'], song['label'])\n",
    "\n",
    "notHitSongsInDecades = {\n",
    "    '10': {},\n",
    "    '20': {},\n",
    "    '60': {},\n",
    "    '70': {},\n",
    "    '80': {},\n",
    "    '90': {},\n",
    "    '00': {},\n",
    "}\n",
    "for song in parsedNotHitSongs:\n",
    "    year = song['features']['releaseYear']    \n",
    "    if year >= '1960':\n",
    "        # Dropping songs released before 60s\n",
    "        \n",
    "        # Check that the song is not actually a hit song\n",
    "        if song['spotifyID'] not in hitSpotiIds:\n",
    "            notHitSongsInDecades[year[2]+\"0\"][song['spotifyID']] = (song['features'], song['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b83b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hitSongsInDecades['10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f68a994",
   "metadata": {},
   "source": [
    "Now that the songs are bucketed based on the release year and duplicates have been removed (within hit/noHits and between noHits and hits), sampling can be applied.\n",
    "\n",
    "To balance the dataset more, the decades are sampled evenly using the smallest amount for songs in a decade (~1000). So from every hit and not hit decade a ~1000 songs are taken randomly to act as the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample as rndsample\n",
    "from numpy import ndarray\n",
    "from typing import Literal\n",
    "\n",
    "from data.types.process import ModelFeatures\n",
    "\n",
    "sampleInstance = tuple[ModelFeatures, int]\n",
    "\n",
    "def getSampleSize(sampleData):\n",
    "    # The sample size is taken from decade with smallest song amount\n",
    "    return min(len(values) for key, values in sampleData.items())\n",
    "\n",
    "def handleFeatures(sampleSongs: list[sampleInstance]):\n",
    "    formatted = []\n",
    "    for featuresAndLabel in sampleSongs:\n",
    "        features, label = featuresAndLabel\n",
    "        formatted.append(([float(f) for f in list(features.values())], label))\n",
    "    return formatted\n",
    "\n",
    "def takeSamplesFromDecades(songsInDecades: dict, sampleSize: int) -> list[sampleInstance]:\n",
    "    # For every decade\n",
    "    samples = []\n",
    "    for decade, songs in songsInDecades.items():\n",
    "        samples = samples + handleFeatures(rndsample(list(songs.values()), sampleSize))\n",
    "    return samples\n",
    "        \n",
    "hitSongSamples = takeSamplesFromDecades(hitSongsInDecades, getSampleSize(hitSongsInDecades))\n",
    "# Note: the sample size is from min hit song size so the hit and not hit song datasets are balanced\n",
    "notHitSongSamples = takeSamplesFromDecades(notHitSongsInDecades, getSampleSize(hitSongsInDecades))\n",
    "print(len(hitSongSamples), len(notHitSongSamples))\n",
    "\n",
    "# The dataset can now be combined\n",
    "songSamples = hitSongSamples + notHitSongSamples\n",
    "print(len(songSamples))\n",
    "print(songSamples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b883cc5",
   "metadata": {},
   "source": [
    "Above is one instance of the sampled data. The numeric and string (year) values were converted to floats but this is not enough. Machine learning models usually perform best when the data is in the same scale. For example now the duration field would have an huge impact on the model as the value is much bigger than the rest.\n",
    "\n",
    "To apply scaling scikit learn has a class to perform this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff44e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import array as nparray, append as npappend, set_printoptions as npset_printoptions\n",
    "\n",
    "npset_printoptions(suppress=True)\n",
    "\n",
    "for i, sample in enumerate(songSamples):\n",
    "    if i == 0:\n",
    "        x = [nparray(sample[0], dtype='float32')]\n",
    "        y = [nparray(nparray(sample[1], dtype='float32'))]\n",
    "        continue\n",
    "        \n",
    "    x = npappend(x, [nparray(sample[0], dtype='float32')], axis=0)\n",
    "    y = npappend(y, [nparray(nparray(sample[1], dtype='float32'))], axis=0)\n",
    "\n",
    "\n",
    "print(x.shape)\n",
    "# Make a multidimensional array\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x)\n",
    "print(\"MAX\")\n",
    "print(scaler.data_max_)\n",
    "print(\"MIN\")\n",
    "print(scaler.data_min_)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Let's see how the scaling affect the features\n",
    "print(x[:5])\n",
    "print(scaler.transform(x[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02560f2",
   "metadata": {},
   "source": [
    "Above max and min values are printed among the dataset. Also a sample of 5 is transformed to illustrate the transformation.\n",
    "\n",
    "Some more insights can be displayed with scipy describe function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26326e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import describe\n",
    "\n",
    "desc_x = describe(x)\n",
    "\n",
    "for i in range(len(x[0])):\n",
    "    f = i+1\n",
    "    print(\"Feature %d \"% f)\n",
    "    print(\"Min: \", desc_x.minmax[0][i])\n",
    "    print(\"Max: \", desc_x.minmax[1][i])\n",
    "    print(\"Mean: \", desc_x.mean[i])\n",
    "    print(\"Variance: \", desc_x.variance[i])\n",
    "    print(\"Skewness: \", desc_x.skewness[i])\n",
    "    print(\"Kurtosis: \", desc_x.kurtosis[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b6dbbe",
   "metadata": {},
   "source": [
    "Now the data can be transformed with the scaler and split into train, validation and test datasets.\n",
    "\n",
    "Scikit learns train_test_split will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ba81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "x, y = shuffle(x, y)\n",
    "transformed_x = scaler.transform(x)\n",
    "# Dataset to train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(transformed_x, y, test_size=0.33, stratify=y)\n",
    "# Split train to validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, stratify=y_train)\n",
    "\n",
    "print(\"Training set\")\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(\"Test set\")\n",
    "print(x_test.shape, y_test.shape)\n",
    "print(\"Validation set\")\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e027933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from numpy import save as npsave\n",
    "startPath = '../data/datasets/readyDatasets/'\n",
    "\n",
    "if not Path(startPath).exists():\n",
    "    Path(startPath).mkdir()\n",
    "\n",
    "# Save the datasets\n",
    "with open(startPath + 'trainX.npy', 'wb') as f:\n",
    "    npsave(f, x_train)\n",
    "with open(startPath + 'trainY.npy', 'wb') as f:\n",
    "    npsave(f, y_train)\n",
    "    \n",
    "with open(startPath + 'valX.npy', 'wb') as f:\n",
    "    npsave(f, x_val)\n",
    "with open(startPath + 'valY.npy', 'wb') as f:\n",
    "    npsave(f, y_val)\n",
    "\n",
    "with open(startPath + 'testX.npy', 'wb') as f:\n",
    "    npsave(f, x_test)\n",
    "with open(startPath + 'testY.npy', 'wb') as f:\n",
    "    npsave(f, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc755c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load as npload\n",
    "\n",
    "startPath = '../data/datasets/readyDatasets/'\n",
    "\n",
    "# And load datasets\n",
    "with open(startPath + 'trainX.npy', 'rb') as f:\n",
    "    x_train = npload(f)\n",
    "with open(startPath + 'trainY.npy', 'rb') as f:\n",
    "    y_train = npload(f)\n",
    "    \n",
    "with open(startPath + 'valX.npy', 'rb') as f:\n",
    "    x_val = npload(f)\n",
    "with open(startPath + 'valY.npy', 'rb') as f:\n",
    "    y_val = npload(f)\n",
    "\n",
    "with open(startPath + 'testX.npy', 'rb') as f:\n",
    "    x_test = npload(f)\n",
    "with open(startPath + 'testY.npy', 'rb') as f:\n",
    "    y_test = npload(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20f05c",
   "metadata": {},
   "source": [
    "Everything is now set to feed to the model.\n",
    "\n",
    "Scikit learns ready MLP model will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e369ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50), max_iter=200).fit(x_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp.score(x_train, y_train))\n",
    "print(\"Validation set score: %f\" % mlp.score(x_val, y_val))\n",
    "print(\"Test set score: %f\" % mlp.score(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
