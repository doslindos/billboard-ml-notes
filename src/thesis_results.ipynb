{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dddf6507-cc09-4455-a6dd-e37b661fa1a5",
   "metadata": {},
   "source": [
    "# Main index\n",
    "\n",
    "[Collect the data](#collect)\n",
    "<br />[Analyse the gathered data](#analysis)\n",
    "<br />[Preprocess the data](#preprocess)\n",
    "<br />[Analyse the processed data](#pro_analysis)\n",
    "<br />[Model the data](#modelling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24c6f4-9e14-4ece-b649-46dfac8c67ac",
   "metadata": {},
   "source": [
    "Environment set ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c3f93-6edc-4ad0-8b4c-75a6f1341eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set up the environment. Code sources are in folders which are in the parent folder of this notebooks scope.\n",
    "#import sys; sys.path.insert(0, '..') # add parent folder path, now files are queriable from parent folder\n",
    "\n",
    "# Install Kaggle api package (not included in the docker image) and spotipy\n",
    "!pip install kaggle spotipy pandas\n",
    "\n",
    "# Install packages for fuzzywuzzy\n",
    "# This is used to calculate ratio between two uneven string\n",
    "%conda install -c conda-forge python-levenshtein\n",
    "%pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c598af9-bc0f-4441-9250-35e3a4fe0332",
   "metadata": {},
   "source": [
    "# <a id='collect'></a>Collecting the data\n",
    "\n",
    "### Index\n",
    "\n",
    "[Kaggle and billboard data](#kaggle)\n",
    "\n",
    "[Spotify and audio features](#spotify)\n",
    "<br />[Spotify song query and matching](#spotify_matching)\n",
    "<br />[Spotify song features for billboard songs](#spotify_features)\n",
    "<br />[Spotify collecting the not hits](#spotify_not_hits)\n",
    "<br />[Spotify filter hit and not hit songs](#spotify_filter)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Credential requirements are explained [here](notes/data.ipynb#dataset_requirements)\n",
    "<br />The credentials go [here](config/env.ini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a04c31-bf2f-4541-959f-6f30cecf861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports + initialize the Spotify API\n",
    "from data.query.util import initializeSpotifyAPI, saveJson\n",
    "api = initializeSpotifyAPI()\n",
    "\n",
    "from data.query.billboard import getBillboardData\n",
    "from data.query.spotify_api import (\n",
    "    getSpotifyDataFromBillboardSongs, \n",
    "    getSpotifyAudioFeatures, \n",
    "    getSongsWithAlbums\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535058c9-ddea-4e39-ba53-4559703f72ee",
   "metadata": {},
   "source": [
    "## <a id='kaggle'></a>Kaggle and billboard data\n",
    "\n",
    "First query the dataset from Kaggle. https://www.kaggle.com/dhruvildave/billboard-the-hot-100-songs\n",
    "[**downloadBillboardData**](data/query/billboard.py) (line 29) is quite straight forward, it takes in the name of the dataset as a string handles the Kaggle credentials and calls [**downloadKaggleDataset**](data/util.py) (line 52) which will download the dataset in the defined path.\n",
    "\n",
    "Default path to store the dataset is **data/datasets/billboard/** and it is created during the data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974339aa-0099-450b-8929-4182b36f92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.query.billboard import downloadBillboardData\n",
    "\n",
    "# Download the kaggle billboard dataset\n",
    "datasetName = 'dhruvildave/billboard-the-hot-100-songs'\n",
    "downloadBillboardData(datasetName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400ebde-7c02-4c0b-b70f-dcf6ff5a829b",
   "metadata": {},
   "source": [
    "Now the billboard songs are queryable with [**getBillboardData**](data/query/billboard.py) (line 10) function. It reads the zip file that contains the dataset, takes the file that is given as input (default charts.csv) and parses the required data. Finally it returns list of billboard songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f7313-1b7f-40de-b565-a77eaf10a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch billboard song data from zip file (kaggle ds)\n",
    "billboardTracks = getBillboardData('data/datasets/billboard/billboard-the-hot-100-songs.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc2105f-5b68-40a7-aea1-9e83f6ce2672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View 10 first songs\n",
    "print(billboardTracks[:10]) # Change to Pandas !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7c5484-9366-4200-875b-635d7764f0b0",
   "metadata": {},
   "source": [
    "## <a id='spotify'></a>Spotify and audio features\n",
    "\n",
    "Now that the billboard song data is fetched from Kaggle. Next the songs are needed to map to songs in spotify database.\n",
    "\n",
    "Unfortunately the kaggle billboard dataset doesn't have Spotify IDs ready to use and query the song information.\n",
    "\n",
    "To get the IDs the songs have to be searched first with the song and artist name.<br />[**getSpotifyDataFromBillboardSongs**](data/query/spotify_api.py) (line 146) will do the trick.\n",
    "\n",
    "### Finding the right song\n",
    "\n",
    "Querying the songs is not totally straight forward. The song names can differ in billboard data and spotify data + it can be that the song is not available in spotify (market, artist etc. reasons).\n",
    "\n",
    "Matching the results with billboard data information is done so that first just the song is used as the query string. If the song names match and the billboard data artist is in the list of artists that the song has listed, all good the song is added to the data.<br />[**songMatching**](data/query/spotify_api.py) (line 120)\n",
    "\n",
    "The songs that do not match directly, a new query is made where song and artist name is used as query string. If the tokenized sets of the query string and spotify result song + artists string matches close enough it is added to the data. Mainly this collect songs with unknown characters in song or artists names or song name having for example \"feat. 'some artist' \" in the song name, but in Spotify data the featured artists are put in artist list and then the song names do not match.<br />[**getSpotifyDataFromBillboardSongs**](data/query/spotify_api.py) (line 224)\n",
    "\n",
    "### Song version blacklist\n",
    "\n",
    "This approach raises a second problem which is that in Spotify there are remix, instrumental and or karaoke versions. These songs are not wanted in the data so blacklisting is implemented. All song names containing these are blacklisted and not added (with a few exceptions that are whitelisted).<br />\n",
    "[**checkIfBlackListed**](data/query/spotify_api.py) (line 50, called in songMatching)\n",
    "\n",
    "### Ignoring duplicates\n",
    "\n",
    "The billboard data has top 100 songs per week , therefore a song that is in the top list two weeks in row will be queried two times. During this querying all the duplicates are ignored.<br />[**fetchSongsByNameFromSpotify**](data/query/spotify_api.py) (line 162, implemented via unMatchedIndexes variable)\n",
    "\n",
    "Finally all search results are stored in json file.\n",
    "\n",
    "*Total number songs to query prints total number of songs in billboard dataset, but one song is queried only ones\n",
    "\n",
    "*Takes about 2-3 hours to query the full dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a77f8f5-4016-49fd-86b1-66e4a5adc8ba",
   "metadata": {},
   "source": [
    "### <a id='spotify_matching'></a>Spotify song query and matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eeab9f-3366-4503-ac16-2d730dc3372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query song information from spotify with song names\n",
    "queryResultPath = '../data/datasets/spotify/query_results.json'\n",
    "billboardSpotifyTrackData = getSpotifyDataFromBillboardSongs(\n",
    "    api, \n",
    "    billboardTracks, \n",
    "    savePath=queryResultPath\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f963677-b81d-4a06-a33d-258b21823362",
   "metadata": {},
   "source": [
    "### <a id='spotify_features'></a>Spotify song features for billboard songs\n",
    "The spotify ID information for billboard songs are queried and now the spotify features can be fetched.\n",
    "\n",
    "[**getSpotifyAudioFeatures**](data/query/spotify_api.py) (line 236) will query the audio features from spotify. Results are stored in json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4561875-508f-4c2c-b16a-38fa38e00913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query features for billboard songs\n",
    "hitSongPath = 'data/datasets/spotify/hit_song.json'\n",
    "billboardHitFeatures = getSpotifyAudioFeatures(api, billboardSpotifyTrackData)\n",
    "# Save the results\n",
    "saveJson(billboardHitFeatures, hitSongPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2276cd1-d9d2-4912-8564-e0dc63122059",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(billboardHitFeatures[:5]) # Change to pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6655dbad-9885-4169-8cd1-9c07e647f5f9",
   "metadata": {},
   "source": [
    "This set of songs will act as hit songs in this work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a039544e-36b6-4db3-a311-234f4530076c",
   "metadata": {},
   "source": [
    "### <a id='spotify_not_hits'></a>Spotify collecting the not hits\n",
    "\n",
    "To use supervised machine learing methods, the model needs examples of the data with labels. To find difference between a billboard song (considered as hit song) and a not billboard song (considered as not hit).\n",
    "\n",
    "The model needs the not hit samples to compare the differences. Just random songs could be fetched from the Spotify API that are not on billboard lists and use them. The problem is: what to search for ?\n",
    "\n",
    "Solution for this is to use songs that shares the album. All collected songs from spotify API do have a information about the album they were released (this is not directly the original album and in some cases links to a hit collection). \n",
    "\n",
    "Also when the artist is usually the same, there should not be such a big difference in the spotify audio feture perspective than compairing a random song from a random artist. Therefore the hit and not hit songs are closer to each other in audio feature perspective, which makes finding a line to separate them harder, but would render a model which answers more specifically on what audio features are the difference in songs from billboard hit list featured artists?\n",
    "\n",
    "For implementation of this querying [**getSongsWithAlbums**](data/query/spotify_api.py) (line 303) does it all.\n",
    "\n",
    "First it will take the album id for every billboard song element and query the album information.<br />[**fetchAlbumTracks**](data/query/spotify_api.py) (line 280 onwards)\n",
    "\n",
    "Next a random sample of the tracks is taken from the songs of the album query results.<br />[**fetchAlbumTracks**](data/query/spotify_api.py) (line 288)\n",
    "\n",
    "If the random song is not the hit song used to query the album and the random song is has no blacklisted elements it is collected.<br />[**fetchAlbumTracks**](data/query/spotify_api.py) (line 293)\n",
    "\n",
    "Song information is parsed and the audio features are fetched for the collected random songs.<br />[**getSongsWithAlbums**](data/query/spotify_api.py) (line 312)\n",
    "\n",
    "Finally the results are stored in json file.\n",
    "\n",
    "Using 5 random songs for every album where every unique billboard song has an album (about 20k) this is going to take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee34db-2c58-4055-b07e-fd1f2fca24d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query non hit songs with album information\n",
    "numSongsFromAlbum = 5\n",
    "notHitSongPath = '../data/datasets/spotify/not_hit_song.json'\n",
    "billboardNOTHitFeatures = getSongsWithAlbums(api, billboardSpotifyTrackData, numSongsFromAlbum)\n",
    "saveJson(billboardNOTHitFeatures, notHitSongPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ee4a4-2dea-41ea-ae67-aac58cb51cee",
   "metadata": {},
   "source": [
    "### <a id='spotify_filter'></a>Spotify filter hit and not hit songs\n",
    "\n",
    "Finally the data can be checked for duplicates and data with no audio features. \n",
    "\n",
    "Duplicates are a hit song in not hit song dataset. These will be removed.\n",
    "\n",
    "Also for sanity check, if a song do not have spotify song features it will be removed from the final data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f0691-077c-43fc-95b7-67e8fc1f30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.query.util import loadJson\n",
    "\n",
    "hitSongPath = 'data/datasets/spotify/hit_song.json'\n",
    "notHitSongPath = 'data/datasets/spotify/not_hit_song.json'\n",
    "\n",
    "# First load the saved data\n",
    "hits = loadJson(hitSongPath)\n",
    "notHits = loadJson(notHitSongPath)\n",
    "\n",
    "print(f\"Hit songs: {len(hits)}\")\n",
    "print(f\"Not Hit songs: {len(notHits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e2de2c-045d-40ec-b1fe-3bd731ef28cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are songs without features and create id list\n",
    "removeFromHits = []\n",
    "# Check if not hit songs share hit song ids\n",
    "hitSpotifyIds = set()\n",
    "for i, hit in enumerate(hits):\n",
    "    hitSpotifyIds.add(hit['info']['spotifyData']['songID'])\n",
    "    \n",
    "    # So if the features is none, one of the values is none or all the values are zeros the song will be added to the list\n",
    "    if hit['features'] == None or all(value == 0 for value in hit['features'].values()):\n",
    "        removeFromHits.append(i)\n",
    "\n",
    "print(f\"Empty features: {len(removeFromHits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c450a256-238e-4404-9f58-c23e10b2d157",
   "metadata": {},
   "source": [
    "Just two empty feature sets in all hit songs. Not bad. These will be removed in a bit.\n",
    "\n",
    "In last cell the set of all hit song spotify ids was created, so now the not hit song ids can be checked not to actually be hit songs.\n",
    "\n",
    "Also if the features are null they are added to the remove list like hits before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40821ab6-03ff-4137-84ef-7817258a534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "removeFromNotHits = []\n",
    "for i, notHit in enumerate(notHits):\n",
    "    if notHit['info']['spotifyData']['songID'] in hitSpotifyIds or notHit['features'] == None or all(value == 0 for value in notHit['features'].values()):\n",
    "        removeFromNotHits.append(i)\n",
    "\n",
    "print(f\"Empty features or actually hits: {len(removeFromNotHits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26611905-cf7f-468e-8253-5131a8b354b7",
   "metadata": {},
   "source": [
    "Almost 10214 songs! It was expected to be a high number as the songs were randomly taken from albums. Let's say a hit song is registered to an album with top hit tracks there's a good possibility to take songs that will appear in the billboard list.\n",
    "\n",
    "Now the empty feature or duplicate songs will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8456b2-0f30-4eef-be10-40bbe0b8bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete empty feature hits, reverse so the order is not messed up\n",
    "for deleteId in list(reversed(removeFromHits)):\n",
    "    hits.pop(deleteId)\n",
    "    \n",
    "# same for not hits\n",
    "for deleteId in list(reversed(removeFromNotHits)):\n",
    "    notHits.pop(deleteId)\n",
    "\n",
    "print(f\"Hit songs: {len(hits)}\")\n",
    "print(f\"Not Hit songs: {len(notHits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f9579-a5fc-4fc8-9a74-1d8e1d3f8927",
   "metadata": {},
   "source": [
    "Alright now just store the data like before, the previous data will be overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd4706c-f0b8-4428-9185-fd6febc9a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveJson(hits, hitSongPath)\n",
    "saveJson(notHits, notHitSongPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65081fd3-2016-4b53-94be-af44bcfe0006",
   "metadata": {},
   "source": [
    "All the needed data is now stored in JSON files. These are processed in the next section to create the datasets that the model will use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a57f5c4-e13f-4976-9860-2400f7d2b9ab",
   "metadata": {},
   "source": [
    "# <a id='analysis'></a> Analyse the gathered data\n",
    "\n",
    "Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d142c-f7dd-4fc5-868b-2fa40b157665",
   "metadata": {},
   "source": [
    "Collection part left us with a dataset that looks like this in the percpective of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547356e-2260-4333-9729-03c9ec3177b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.plotting.util import makeHistogram\n",
    "\n",
    "# Here is the number of hits and not hits\n",
    "makeHistogram([0, 1], [len(notHits), len(hits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71529b5-b274-4e11-82ce-55c6c2397e4a",
   "metadata": {},
   "source": [
    "Song amounts by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6a04a-8d81-4f6c-94fc-90299f1f8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, year in enumerate(sorted(hitsDataset['year'].unique())):\n",
    "    print(f\"Year: {year} Number of songs: {len(hitsDataset[hitsDataset['year'] == year])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3b866-8eda-493d-b19e-1adc505b2fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0865c8c2-77e2-495f-865b-15ca2fda54ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f2f59-9a99-4ea5-92e1-2dc292e64b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e048abd2-e847-4faa-ae82-202cbf27ea9c",
   "metadata": {},
   "source": [
    "# <a id='preprocess'></a> Song data preprocessing\n",
    "\n",
    "Index\n",
    "\n",
    "[Load the data](#load)\n",
    "</br>[Add labels](#process_label)\n",
    "</br>[Sample by year](#process_sample)\n",
    "</br>[Fit transformation scaler](#process_fit_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ff977-c2c2-4744-a409-d87b2bbb3e9a",
   "metadata": {},
   "source": [
    "### <a id='process_load'></a>Load the gathered data\n",
    "\n",
    "First step is to load the previously gathered data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985910f-976c-44ce-a865-6bf0fc10ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the downloaded data\n",
    "from data.query.util import loadJson\n",
    "\n",
    "notHitSongPath = '../data/datasets/spotify/not_hit_song.json'\n",
    "hitSongPath = '../data/datasets/spotify/hit_song.json'\n",
    "\n",
    "# Load the data\n",
    "hits = loadJson(hitSongPath)\n",
    "notHits = loadJson(notHitSongPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8421c0-ccc2-4ec4-86b3-679b648ea9af",
   "metadata": {},
   "source": [
    "The hits data will be collected and stored in a pandas dataframe for further sampling and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e3290b-c52b-4639-9e25-952095b78700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from data.process.util import parseYearFromDate\n",
    "\n",
    "features = []\n",
    "for hit in hits:\n",
    "    year = parseYearFromDate(hit['info']['spotifyData']['album']['releaseDate'])\n",
    "    features.append({**hit['features'], 'year': year[2:]})\n",
    "\n",
    "hitsDataset = DataFrame.from_records(features)\n",
    "hitsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb87af1-15cc-4084-866f-545469e5f69a",
   "metadata": {},
   "source": [
    "Now the same to not hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea2c28-e6c6-4812-a003-a1b94f9766df",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for notHit in notHits:\n",
    "    year = parseYearFromDate(notHit['info']['spotifyData']['album']['releaseDate'])\n",
    "    features.append({**notHit['features'], 'year': year[2:]})\n",
    "\n",
    "notHitsDataset = DataFrame.from_records(features)\n",
    "notHitsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351cd5a4-f7f5-4fa9-a7de-e086c323acb0",
   "metadata": {},
   "source": [
    "### <a id='process_label'></a>Add labels\n",
    "\n",
    "The original collected dataset of hits **hitsDataset** and not hits **notHitsDataset** are loaded and ready for further manipulations.\n",
    "\n",
    "But as a first step the labels can be added to the datasets as a feature.\n",
    "\n",
    "That is easy, for all hits a label value of 1 and not hits a label value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f4da1-45ff-4228-92a8-1e399b98ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now labels in place\n",
    "hitsDataset['label'] = 1\n",
    "notHitsDataset['label'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03186d0f-1f3f-4e99-8d50-35419b7cf16f",
   "metadata": {},
   "source": [
    "eghtyFiveSampleSize = 250\n",
    "if int(year) > 85 or int(year) < 22 <- 85 logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0595fc62-1946-41e4-98cf-d1bb4e2cd7a4",
   "metadata": {},
   "source": [
    "### <a id='process_sample'></a>Sample the data by year\n",
    "\n",
    "Dataset is balanced with [**sampleByYears**](data/process/balance.py).\n",
    "\n",
    "All unique years in the dataset is looped and the first step is to check wheter the currently processed year is in between the ones that are wanted. The only interest is for songs between 1965 and 2021, so years not between those will be skipped.\n",
    "\n",
    "Next all the songs are taken randomly and added to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18fcd3e9-dd56-43cb-ba8f-ec7245627dcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocess\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbalance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sampleByYears\n\u001b[1;32m      3\u001b[0m sampleSize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m192\u001b[39m\n\u001b[1;32m      4\u001b[0m balancedHitDataset, balancedNotHitDataset \u001b[38;5;241m=\u001b[39m sampleByYears(\n\u001b[1;32m      5\u001b[0m                                                 hitsDataset, \n\u001b[1;32m      6\u001b[0m                                                 notHitsDataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                 \u001b[38;5;241m21\u001b[39m\n\u001b[1;32m     10\u001b[0m                                             )\n",
      "File \u001b[0;32m/src/data/process/balance.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concat, DataFrame\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msampleByYears\u001b[39m(\n\u001b[0;32m----> 5\u001b[0m     hits: \u001b[43mDataset\u001b[49m,\n\u001b[1;32m      6\u001b[0m     notHits: Dataset,\n\u001b[1;32m      7\u001b[0m     sampleSize: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m      8\u001b[0m     earliest: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m      9\u001b[0m     latest: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m     10\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[DataFrame]:\n\u001b[1;32m     12\u001b[0m     hitSamples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     notHitSamples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from data.process.balance import sampleByYears\n",
    "\n",
    "sampleSize = 192\n",
    "balancedHitDataset, balancedNotHitDataset = sampleByYears(\n",
    "                                                hitsDataset, \n",
    "                                                notHitsDataset,\n",
    "                                                sampleSize,\n",
    "                                                65,\n",
    "                                                21\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa09a61-0fa2-488b-b2f7-1a2b97396466",
   "metadata": {},
   "source": [
    "### <a id='process_fit_transform'></a>Apply transformations\n",
    "\n",
    "The data needs some transformations.\n",
    "\n",
    "Some features are scaled to be between 0 and 1.\n",
    "\n",
    "Categorical features are oneHot encoded.\n",
    "\n",
    "And finally the scaler/transformer is fitted with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175020f7-fbe8-49a6-b842-758942eefd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# First the balanced dataset needs to be concatenated\n",
    "dataset = concat([balancedHitDataset, balancedNotHitDataset], ignore_index=True)\n",
    "\n",
    "# Initialize the transformer\n",
    "transformer = ColumnTransformer([ \n",
    "    (\"scale\", MinMaxScaler(), ['durationMS', 'loudness', 'tempo', 'year']),\n",
    "    (\"onehot\", OneHotEncoder(), ['timeSignature', 'key', 'mode'])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "# Fit the transformer with the whole sampled dataset\n",
    "transformer.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9177d1d7-235f-482a-8e4a-74fa729e7521",
   "metadata": {},
   "source": [
    "# <a id='pro_analysis'></a> Processed data analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec7443-389d-480a-984e-693aa20992f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6ddca-5cf4-439b-87f2-72b6ae5d365e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3fbdfa0-a08a-48ae-8aed-510d25fdbdc7",
   "metadata": {},
   "source": [
    "# <a id='modelling'></a> Modelling\n",
    "\n",
    "Index\n",
    "\n",
    "[Initial stuff](#model_init)\n",
    "</br>[Logistic Regression](#logreg)\n",
    "</br>[Support Vector Machine](#svm)\n",
    "</br>[Multi-Layer Perceptron](#MLP)\n",
    "</br>[Random Forest](#rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3bd06d-ab1e-49f6-9ac5-2fdb6da38e0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <a id='model_init'></a>Initial stuff\n",
    "\n",
    "Most of the models need hyperparameters to be set. As the values for them are not totally clear, it is common to try multiple and use the best model measured by some performance measure.\n",
    "\n",
    "Scikit-learn GridSearch will be used to search good hyperparameters. It takes in as parameters all the combinations of hyperparameters to test. It creates and fits the model one by one with all of the hyperparameter combinations. The best performing model is then usable as a result.\n",
    "\n",
    "Another instrument in play here is the Group K-fold scikit-learn object. This is used to apply a cross validation on the training.\n",
    "\n",
    "Maybe more info here....\n",
    "\n",
    "Also the data needs to be transformed. In the [section](#process_fit_transform) where transformer/scaler was fitted, is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326506ac-15ba-4a49-ab20-e3b45860e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "groups = dataset.label\n",
    "group_kfold = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac381ab9-3d59-4e40-a7f8-12af0c639b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = transformer.transform(dataset.drop(['label'], axis=1))\n",
    "labels = dataset.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b057fbc-c6aa-4f2c-b6d1-048a00196d3b",
   "metadata": {},
   "source": [
    "### <a id='logreg'></a>Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f24f2-02cb-4242-88ef-37f57f283b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "params = [\n",
    "    {\n",
    "        'solver': ['liblinear'], \n",
    "        'penalty': ['l1', 'l2'],  \n",
    "        'multi_class': ['ovr'],\n",
    "        'max_iter': [100, 250, 500, 1000],\n",
    "        'C': [0.01, 0.1, 1]\n",
    "    }, \n",
    "    {\n",
    "        'solver': ['saga'], \n",
    "        'penalty': ['elasticnet'], \n",
    "        'multi_class': ['ovr'], \n",
    "        'n_jobs': [-1], \n",
    "        'warm_start': [True],\n",
    "        'max_iter': [1000, 10000, 100000],\n",
    "        'C': [0.01, 0.1, 1],\n",
    "        'l1_ratio': [0.4, 0.5, 0.6]\n",
    "    }, \n",
    "]\n",
    "\n",
    "# Classifier trained with dataset One\n",
    "logReg = GridSearchCV(LogisticRegression(), params, n_jobs=-1, scoring='accuracy')\n",
    "logReg.fit(transformed_data, labels, groups=groups)\n",
    "\n",
    "print(f\"Best results: {logReg.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44ec40-a9bc-4ff8-93e5-3638d6f4e3a3",
   "metadata": {},
   "source": [
    "### <a id='svm'></a>Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4562d575-57bd-413f-9eab-b3766018910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "params = [\n",
    "    {\n",
    "        'kernel': ['linear', 'rbf'], \n",
    "        'class_weight': [None, 'balanced'],\n",
    "        'max_iter': [-1],\n",
    "        'C': [2.0, 3.0, 10.0],\n",
    "        'gamma': [0.1, 'auto']\n",
    "    },\n",
    "]\n",
    "\n",
    "# Classifier trained with dataset One\n",
    "svm = GridSearchCV(SVC(), params, n_jobs=-1, scoring='accuracy')\n",
    "svm.fit(transformed_data, labels, groups=groups)\n",
    "\n",
    "print(f\"Best results: {svm.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e6ff1-42b3-438d-8870-e9f7b15effa0",
   "metadata": {},
   "source": [
    "### <a id='MLP'></a>Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be3d3f-d22d-43ea-8d05-557fdb4ff272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "params = [\n",
    "    {\n",
    "        'hidden_layer_sizes': [\n",
    "            (10,),\n",
    "            (20,),\n",
    "        ], \n",
    "        'activation': ['relu', 'logistic'],  \n",
    "        'solver': ['lbfgs', 'adam'],\n",
    "        'max_iter': [10000],\n",
    "        'alpha': [0.0001, 0.001],\n",
    "        'early_stopping': [True]\n",
    "    }, \n",
    "]\n",
    "\n",
    "# Classifier trained with dataset One\n",
    "MLP = GridSearchCV(MLPClassifier(), params, n_jobs=-1, scoring='accuracy')\n",
    "MLP.fit(transformed_data, labels, groups=groups)\n",
    "\n",
    "print(f\"Best results: {MLP.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc744126-f128-452f-971b-26c6433d26ed",
   "metadata": {},
   "source": [
    "### <a id='rf'></a>Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25975372-87cf-400e-abae-c7f93d92b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "params = [\n",
    "    {\n",
    "        'n_estimators': [800, 1600],\n",
    "        'n_jobs': [-1],\n",
    "        'max_features': [8, 10, 'auto']\n",
    "    },\n",
    "]\n",
    "\n",
    "# Classifier trained with dataset One\n",
    "forest = GridSearchCV(RandomForestClassifier(), params, n_jobs=-1, scoring='accuracy')\n",
    "forest.fit(transformed_data, labels, groups=groups)\n",
    "\n",
    "\n",
    "print(f\"Best results: {forestOne.best_score_}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
